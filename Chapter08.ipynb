{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第8章: 機械学習\n",
    "\n",
    "本章では，Bo Pang氏とLillian Lee氏が公開している[Movie Review Data](http://www.cs.cornell.edu/people/pabo/movie-review-data/)の[sentence polarity dataset v1.0](http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.README.1.0.txt)を用い，文を肯定的（ポジティブ）もしくは否定的（ネガティブ）に分類するタスク（極性分析）に取り組む．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70. データの入手・整形\n",
    "[文に関する極性分析の正解データ](http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz)を用い，以下の要領で正解データ（sentiment.txt）を作成せよ．\n",
    "\n",
    "rt-polarity.posの各行の先頭に\"+1 \"という文字列を追加する（極性ラベル\"+1\"とスペースに続けて肯定的な文の内容が続く）\n",
    "rt-polarity.negの各行の先頭に\"-1 \"という文字列を追加する（極性ラベル\"-1\"とスペースに続けて否定的な文の内容が続く）\n",
    "上述1と2の内容を結合（concatenate）し，行をランダムに並び替える\n",
    "sentiment.txtを作成したら，正例（肯定的な文）の数と負例（否定的な文）の数を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    5331\n",
      "    5331\n"
     ]
    }
   ],
   "source": [
    "# done using shellscripts.\n",
    "# add '+1' or '-1' by using vim.\n",
    "# using VISUAL BLOCK mode, choose all rows, and then press shift + i, entering intert mode, then finally insety '+1' or '-1' and press esc.\n",
    "# it insets '+1' or '01' to all rows.\n",
    "\n",
    "#!cat rt-polarity.neg rt-polarity.pos | gshuf > sentiment.txt\n",
    "\n",
    "!cat sentiment.txt | grep '^\\+1' | wc -l\n",
    "!cat sentiment.txt | grep '^\\-1' | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 71. ストップワード\n",
    "英語のストップワードのリスト（ストップリスト）を適当に作成せよ．さらに，引数に与えられた単語（文字列）がストップリストに含まれている場合は真，それ以外は偽を返す関数を実装せよ．さらに，その関数に対するテストを記述せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class stopWord:\n",
    "    def __init__(self):\n",
    "        self.words = 'a,able,about,across,after,all,almost,also,am,among,an,and,any,are,as,at,be,because,been,but,by,can,cannot,could,dear,did,do,does,either,else,ever,every,for,from,get,got,had,has,have,he,her,hers,him,his,how,however,i,if,in,into,is,it,its,just,least,let,like,likely,may,me,might,most,must,my,neither,no,nor,not,of,off,often,on,only,or,other,our,own,rather,said,say,says,she,should,since,so,some,than,that,the,their,them,then,there,these,they,this,tis,to,too,twas,us,wants,was,we,were,what,when,where,which,while,who,whom,why,will,with,would,yet,you,your'.lower().split(',')\n",
    "        \n",
    "    \n",
    "    def exists(self, word):\n",
    "        if word.lower() in self.words:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "\n",
    "stopword = stopWord()\n",
    "\n",
    "\n",
    "assert stopword.exists('a')\n",
    "assert stopword.exists('after')\n",
    "assert stopword.exists('among')\n",
    "assert stopword.exists('are')\n",
    "\n",
    "assert not stopword.exists('test')\n",
    "assert not stopword.exists('check')\n",
    "assert not stopword.exists('paper')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 72. 素性抽出\n",
    "極性分析に有用そうな素性を各自で設計し，学習データから素性を抽出せよ．素性としては，レビューからストップワードを除去し，各単語をステミング処理したものが最低限のベースラインとなるであろう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import snowballstemmer\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class stopWord_Stem:\n",
    "    def __init__(self):\n",
    "        self.words = 'a,able,about,across,after,all,almost,also,am,among,an,and,any,are,as,at,be,because,been,but,by,can,cannot,could,dear,did,do,does,either,else,ever,every,for,from,get,got,had,has,have,he,her,hers,him,his,how,however,i,if,in,into,is,it,its,just,least,let,like,likely,may,me,might,most,must,my,neither,no,nor,not,of,off,often,on,only,or,other,our,own,rather,said,say,says,she,should,since,so,some,than,that,the,their,them,then,there,these,they,this,tis,to,too,twas,us,wants,was,we,were,what,when,where,which,while,who,whom,why,will,with,would,yet,you,your'.lower().split(',')\n",
    "        self.interjections = ['!', '?', 'ah', 'ahh', 'ahem', 'aww', 'aw', 'argh', 'bab', 'boo', 'duh', 'eek', 'eh', 'eep', 'gee', 'grr', 'hah', 'hmm', 'haha', 'huh', 'meh', 'ick', 'yuck', 'ich', 'yak', 'meh', 'mhm', 'mm', 'oh', 'iy', 'oy', 'pew', 'uh', 'uhh', 'wow']\n",
    "        self.num = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten']\n",
    "        self.other_word = ['de', 'niro', 'goe']\n",
    "        self.stemmer = snowballstemmer.stemmer('english')\n",
    "        self.stm_list = []\n",
    "        self.features = []\n",
    "        self.word_counter = Counter()\n",
    "\n",
    "    def is_stopWord(self, word):\n",
    "        if word.lower() in self.words:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_interjections(self, word):\n",
    "        if word.lower() in self.interjections:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def is_num(self, word):\n",
    "        if word.lower() in self.num:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_someword(self, word):\n",
    "        if word.lower() in self.other_word:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_features(self, txt):\n",
    "        \n",
    "        for line in txt:\n",
    "            for word in line[3:].split():\n",
    "                \n",
    "                word = word.strip()\n",
    "                \n",
    "                if re.search('[0-9(\\'d)(\\'v)\\/\\[\\]\\-]', word):\n",
    "                    continue\n",
    "                \n",
    "                elif self.is_stopWord(word) or self.is_interjections(word) or self.is_num(word):\n",
    "                    continue\n",
    "                \n",
    "                stm_word = self.stemmer.stemWord(word)\n",
    "                \n",
    "                if stm_word != '!' and stm_word != '?' and len(word) <= 2:\n",
    "                    continue\n",
    "                \n",
    "                if self.is_someword(stm_word):\n",
    "                    continue\n",
    "        \n",
    "    \n",
    "    \n",
    "                \n",
    "                self.word_counter.update([stm_word])\n",
    "        \n",
    "        features = [word for word, count in self.word_counter.items() if count >= 6]\n",
    "        \n",
    "        return features\n",
    "        \n",
    "     \n",
    "    \n",
    "txt_file = 'sentiment.txt'\n",
    "result_file = 'features.txt'\n",
    "\n",
    "process = stopWord_Stem()\n",
    "\n",
    "with codecs.open(txt_file, 'r', 'cp1252') as f:\n",
    "    features = process.get_features(f)\n",
    "\n",
    "with codecs.open(result_file, 'w', 'cp1252') as f:\n",
    "    for i in features:\n",
    "        f.write(i)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 73. 学習\n",
    "72で抽出した素性を用いて，ロジスティック回帰モデルを学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import snowballstemmer\n",
    "import numpy as np\n",
    "\n",
    "    \n",
    "class logit:\n",
    "    def __init__(self, sentiments, features):\n",
    "        \n",
    "        self.words = 'a,able,about,across,after,all,almost,also,am,among,an,and,any,are,as,at,be,because,been,but,by,can,cannot,could,dear,did,do,does,either,else,ever,every,for,from,get,got,had,has,have,he,her,hers,him,his,how,however,i,if,in,into,is,it,its,just,least,let,like,likely,may,me,might,most,must,my,neither,no,nor,not,of,off,often,on,only,or,other,our,own,rather,said,say,says,she,should,since,so,some,than,that,the,their,them,then,there,these,they,this,tis,to,too,twas,us,wants,was,we,were,what,when,where,which,while,who,whom,why,will,with,would,yet,you,your'.lower().split(',')\n",
    "        self.interjections = ['!', '?', 'ah', 'ahh', 'ahem', 'aww', 'aw', 'argh', 'bab', 'boo', 'duh', 'eek', 'eh', 'eep', 'gee', 'grr', 'hah', 'hmm', 'haha', 'huh', 'meh', 'ick', 'yuck', 'ich', 'yak', 'meh', 'mhm', 'mm', 'oh', 'iy', 'oy', 'pew', 'uh', 'uhh', 'wow']\n",
    "        self.num = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten']\n",
    "        self.other_word = ['de', 'niro', 'goe']\n",
    "        self.stemmer = snowballstemmer.stemmer('english')       \n",
    "        \n",
    "        self.features = []\n",
    "        self.sentiments = []\n",
    "        with open(features, 'r') as f:\n",
    "            for word in f:\n",
    "                self.features.append(word.strip())\n",
    "                \n",
    "        with open(sentiments, 'r') as f:\n",
    "            for line in f:\n",
    "                self.sentiments.append(line.strip())\n",
    "        \n",
    "        self.num_features = len(self.features)\n",
    "        self.num_sentiments = len(self.sentiments)\n",
    "        \n",
    "        self.data_x = np.zeros([self.num_sentiments, self.num_features + 1], dtype=np.float64)\n",
    "        self.data_y = np.zeros(self.num_sentiments, dtype=np.float64)\n",
    "        \n",
    "        np.random.seed(1)\n",
    "        self.weights = np.random.randn(self.num_features, dtype=np.float64)\n",
    "        \n",
    "        for i, line in enumerate(sentiments):\n",
    "            \n",
    "            self.data_x[i] = self._extract_features(line[3:])\n",
    "            \n",
    "            if line[0:2] == '+1':\n",
    "                self.data_y[i] = 1\n",
    "        \n",
    "    def is_stopWord(self, word):\n",
    "        if word.lower() in self.words:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_interjections(self, word):\n",
    "        if word.lower() in self.interjections:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def is_num(self, word):\n",
    "        if word.lower() in self.num:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_someword(self, word):\n",
    "        if word.lower() in self.other_word:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    \n",
    "    def _extract_features(self, txt):\n",
    "        data = np.zeros(self.num_features + 1, dtype=np.float64)\n",
    "        data[0] = 1\n",
    "        \n",
    "        for word in txt:\n",
    "            word = word.strip()\n",
    "            if self.is_stopWord(word) or self.is_interjections(word) or self.is_num(word) or self.is_someword(word):\n",
    "                continue\n",
    "        \n",
    "            stm_word = self.stemmer.stemWord(word)\n",
    "            \n",
    "            if stm_word in self.features:\n",
    "                data[self.features.index(stm_word)] = 1\n",
    "\n",
    "        return data\n",
    "                \n",
    "        \n",
    "    def loss(out ,y):\n",
    "        \n",
    "        #res = sigmoid(x.dot(weights))\n",
    "        \n",
    "        return 1\n",
    "        \n",
    "    def sigmoid(x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 74. 予測\n",
    "73で学習したロジスティック回帰モデルを用い，与えられた文の極性ラベル（正例なら\"+1\"，負例なら\"-1\"）と，その予測確率を計算するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 75. 素性の重み\n",
    "73で学習したロジスティック回帰モデルの中で，重みの高い素性トップ10と，重みの低い素性トップ10を確認せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 76. ラベル付け\n",
    "学習データに対してロジスティック回帰モデルを適用し，正解のラベル，予測されたラベル，予測確率をタブ区切り形式で出力せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 77. 正解率の計測\n",
    "76の出力を受け取り，予測の正解率，正例に関する適合率，再現率，F1スコアを求めるプログラムを作成せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 78. 5分割交差検定\n",
    "76-77の実験では，学習に用いた事例を評価にも用いたため，正当な評価とは言えない．すなわち，分類器が訓練事例を丸暗記する際の性能を評価しており，モデルの汎化性能を測定していない．そこで，5分割交差検定により，極性分類の正解率，適合率，再現率，F1スコアを求めよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 79. 適合率-再現率グラフの描画\n",
    "ロジスティック回帰モデルの分類の閾値を変化させることで，適合率-再現率グラフを描画せよ．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
